import os
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from tqdm import tqdm
import logging
import multiprocessing as mp
from functools import partial
import time
import threading

# --- é…ç½® ---
TRAIN_PDB_BASE_DIR = 'biosnap/train_pdb'
WITH_PDBID_CSV_PATH = 'biosnap/train_csv/with_pdbid.csv'

# CSVæ–‡ä»¶ä¸­çš„åˆ—å
PDB_ID_COL = 'PDB_ID'
SMILES_COL = 'SMILES'
SEQID_COL = 'seqid'
FAILURE_LOG_FILE = 'biosnap/sdf_generation_failures_fast.log'

# æ€§èƒ½ä¼˜åŒ–é…ç½®
MAX_WORKERS = 1  # ä½¿ç”¨å•è¿›ç¨‹é¿å…å¡ä½
BATCH_SIZE = 100  # æ‰¹å¤„ç†å¤§å°
SKIP_OPTIMIZATION = True  # è·³è¿‡å‡ ä½•ä¼˜åŒ–ï¼Œä¸ä¹‹å‰æ•°æ®é›†ä¿æŒä¸€è‡´
TIMEOUT_SECONDS = 30  # å•ä¸ªåˆ†å­å¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# è®¾ç½®æ—¥å¿—è®°å½•
logging.basicConfig(filename=FAILURE_LOG_FILE, level=logging.INFO, 
                    format='%(asctime)s - PDB: %(pdb_id)s - SEQID: %(seqid)s - SMILES: %(smiles)s - REASON: %(reason)s',
                    filemode='w')
logger = logging.getLogger(__name__)

def run_with_timeout(func, args, timeout_seconds):
    """åœ¨æŒ‡å®šæ—¶é—´å†…è¿è¡Œå‡½æ•°ï¼Œè¶…æ—¶åˆ™è¿”å›None"""
    result = [None]
    exception = [None]

    def target():
        try:
            result[0] = func(*args)
        except Exception as e:
            exception[0] = e

    thread = threading.Thread(target=target)
    thread.daemon = True
    thread.start()
    thread.join(timeout_seconds)

    if thread.is_alive():
        # è¶…æ—¶äº†
        return None, "timeout"
    elif exception[0]:
        return None, str(exception[0])
    else:
        return result[0], None

def sanitize_name_component(name_component):
    """å¯¹ç”¨äºæ–‡ä»¶åæˆ–ç›®å½•åçš„éƒ¨åˆ†è¿›è¡ŒåŸºæœ¬æ¸…ç†ã€‚"""
    return "".join(char if char.isalnum() or char in ['_', '-'] else '_' for char in str(name_component))

def smiles_to_3d_sdf_fast(smiles_string, sdf_file_path, pdb_id_for_log, seqid_for_log):
    """
    å°†SMILESå­—ç¬¦ä¸²è½¬æ¢ä¸ºå¸¦æœ‰3Dåæ ‡çš„SDFæ–‡ä»¶ã€‚
    ä¿æŒä¸ä¹‹å‰æ•°æ®é›†ä¸€è‡´çš„ç”Ÿæˆæ–¹æ³•ï¼Œç¡®ä¿è´¨é‡ã€‚
    """
    log_extra = {'pdb_id': pdb_id_for_log, 'seqid': seqid_for_log, 'smiles': smiles_string}
    try:
        # ç¦ç”¨RDKitçš„è­¦å‘Šä¿¡æ¯ä»¥å‡å°‘è¾“å‡º
        from rdkit import RDLogger
        RDLogger.DisableLog('rdApp.*')

        print(f"        ğŸ”„ å¼€å§‹å¤„ç†SMILES: {smiles_string[:50]}...")

        mol = Chem.MolFromSmiles(smiles_string)
        if not mol:
            reason = f"æ— æ³•ä»SMILESè§£æåˆ†å­: '{smiles_string}'"
            logger.info(reason, extra={**log_extra, 'reason': "SMILESè§£æå¤±è´¥"})
            print(f"        âŒ SMILESè§£æå¤±è´¥")
            return False

        print(f"        â• æ·»åŠ æ°¢åŸå­...")
        mol = Chem.AddHs(mol)

        print(f"        ğŸ¯ ç”Ÿæˆ3Dæ„è±¡...")
        # ä½¿ç”¨è¶…æ—¶æœºåˆ¶çš„3Dæ„è±¡ç”Ÿæˆ
        def generate_3d_conformation():
            params = AllChem.ETKDGv3()
            params.maxAttempts = 5  # ä¿æŒåˆç†çš„å°è¯•æ¬¡æ•°
            params.numThreads = 1   # å•çº¿ç¨‹é¿å…å†²çª
            params.randomSeed = 42  # å›ºå®šéšæœºç§å­ä¿è¯å¯é‡ç°æ€§

            status = AllChem.EmbedMolecule(mol, params)
            if status == -1:
                # åå¤‡æ–¹æ¡ˆï¼šä½¿ç”¨éšæœºåæ ‡
                status_random = AllChem.EmbedMolecule(mol, useRandomCoords=True)
                return status_random
            return status

        # ä½¿ç”¨30ç§’è¶…æ—¶
        result, error = run_with_timeout(generate_3d_conformation, (), TIMEOUT_SECONDS)

        if error == "timeout":
            reason = f"3Dæ„è±¡ç”Ÿæˆè¶…æ—¶ ({TIMEOUT_SECONDS}ç§’)"
            logger.info(reason, extra={**log_extra, 'reason': reason})
            print(f"        â° 3Dæ„è±¡ç”Ÿæˆè¶…æ—¶ï¼Œè·³è¿‡")
            return False
        elif error:
            reason = f"3Dæ„è±¡ç”Ÿæˆå¼‚å¸¸: {error}"
            logger.info(reason, extra={**log_extra, 'reason': reason})
            print(f"        âŒ 3Dæ„è±¡ç”Ÿæˆå¼‚å¸¸: {error}")
            return False
        elif result == -1:
            reason = f"3Dåæ ‡ç”Ÿæˆå¤±è´¥"
            logger.info(reason, extra={**log_extra, 'reason': "3Dåæ ‡ç”Ÿæˆå¤±è´¥"})
            print(f"        âŒ 3Dåæ ‡ç”Ÿæˆå¤±è´¥")
            return False

        # è¿›è¡Œå‡ ä½•ä¼˜åŒ–ï¼ˆä¿æŒä¸ä¹‹å‰æ•°æ®é›†ä¸€è‡´ï¼‰
        if not SKIP_OPTIMIZATION:
            print(f"        âš™ï¸ è¿›è¡Œå‡ ä½•ä¼˜åŒ–...")

            def optimize_geometry():
                return AllChem.UFFOptimizeMolecule(mol, maxIters=200)

            # ä½¿ç”¨è¶…æ—¶æœºåˆ¶è¿›è¡Œå‡ ä½•ä¼˜åŒ–
            opt_result, opt_error = run_with_timeout(optimize_geometry, (), TIMEOUT_SECONDS)

            if opt_error == "timeout":
                print(f"        â° å‡ ä½•ä¼˜åŒ–è¶…æ—¶ï¼Œè·³è¿‡ä¼˜åŒ–æ­¥éª¤")
                # è¶…æ—¶ä¸å½±å“æ•´ä½“æµç¨‹ï¼Œç»§ç»­ä½¿ç”¨æœªä¼˜åŒ–çš„ç»“æ„
            elif opt_error:
                print(f"        âš ï¸ å‡ ä½•ä¼˜åŒ–å¼‚å¸¸: {opt_error}")
                # ä¼˜åŒ–å¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹
            else:
                print(f"        âœ… å‡ ä½•ä¼˜åŒ–å®Œæˆ")
                # ä¼˜åŒ–æˆåŠŸ

        print(f"        ğŸ’¾ å†™å…¥SDFæ–‡ä»¶...")
        # å†™å…¥SDFæ–‡ä»¶
        writer = Chem.SDWriter(sdf_file_path)
        writer.write(mol)
        writer.close()
        print(f"        âœ… SDFæ–‡ä»¶ç”ŸæˆæˆåŠŸ")
        return True

    except Exception as e:
        reason = f"å¤„ç†å¼‚å¸¸: {str(e)}"
        logger.info(reason, extra={**log_extra, 'reason': reason})
        print(f"        âŒ å¤„ç†å¼‚å¸¸: {str(e)}")
        return False

def process_ligand_batch(ligand_batch, batch_num):
    """å¤„ç†ä¸€æ‰¹é…ä½“æ•°æ®"""
    results = {
        'success': 0,
        'failed': 0,
        'skipped': 0
    }

    for i, ligand_data in enumerate(ligand_batch):
        pdb_id, seqid, smiles, sdf_path = ligand_data

        # æ˜¾ç¤ºå½“å‰å¤„ç†çš„æ–‡ä»¶
        print(f"    [{batch_num}:{i+1}] å¤„ç† {pdb_id}_{seqid}")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(sdf_path):
            print(f"        â© è·³è¿‡ (æ–‡ä»¶å·²å­˜åœ¨): {sdf_path}")
            results['skipped'] += 1
            continue

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(sdf_path), exist_ok=True)

        # ç”ŸæˆSDFæ–‡ä»¶
        if smiles_to_3d_sdf_fast(smiles, sdf_path, pdb_id, seqid):
            print(f"        âœ… æˆåŠŸç”Ÿæˆ: {sdf_path}")
            results['success'] += 1
        else:
            print(f"        âŒ ç”Ÿæˆå¤±è´¥: {pdb_id}_{seqid} - SMILES: {smiles[:50]}...")
            results['failed'] += 1

    return results

def main():
    print(f"ğŸš€ å¯åŠ¨å¿«é€ŸSDFç”Ÿæˆè„šæœ¬...")
    print(f"ğŸ“Š é…ç½®: æœ€å¤§è¿›ç¨‹æ•°={MAX_WORKERS}, æ‰¹å¤„ç†å¤§å°={BATCH_SIZE}")
    print(f"âš¡ æ€§èƒ½ä¼˜åŒ–: è·³è¿‡å‡ ä½•ä¼˜åŒ–={SKIP_OPTIMIZATION}")
    print(f"ğŸ“ å¤±è´¥æ—¥å¿—: {FAILURE_LOG_FILE}")
    
    # æ£€æŸ¥åŸºæœ¬ç›®å½•
    if not os.path.isdir(TRAIN_PDB_BASE_DIR):
        print(f"âŒ é”™è¯¯: åŸºç¡€PDBç›®å½• '{TRAIN_PDB_BASE_DIR}' æœªæ‰¾åˆ°")
        return
    
    if not os.path.isfile(WITH_PDBID_CSV_PATH):
        print(f"âŒ é”™è¯¯: CSVæ–‡ä»¶ '{WITH_PDBID_CSV_PATH}' æœªæ‰¾åˆ°")
        return

    try:
        print(f"ğŸ“– æ­£åœ¨åŠ è½½CSVæ–‡ä»¶: {WITH_PDBID_CSV_PATH}...")
        df_with_pdbid = pd.read_csv(
            WITH_PDBID_CSV_PATH,
            dtype={PDB_ID_COL: str, SEQID_COL: str, SMILES_COL: str}
        )
        print(f"âœ… ä»CSVåŠ è½½äº† {len(df_with_pdbid)} æ¡è®°å½•")
        
        # æ•°æ®æ¸…ç†
        df_with_pdbid[PDB_ID_COL] = df_with_pdbid[PDB_ID_COL].str.upper().str.strip()
        df_with_pdbid[SMILES_COL] = df_with_pdbid[SMILES_COL].astype(str).str.strip()
        df_with_pdbid[SEQID_COL] = df_with_pdbid[SEQID_COL].astype(str).str.strip()
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: åŠ è½½CSVæ–‡ä»¶å¤±è´¥: {e}")
        return

    # è·å–PDBæ–‡ä»¶å¤¹
    try:
        pdb_id_folders = [
            folder for folder in os.listdir(TRAIN_PDB_BASE_DIR)
            if os.path.isdir(os.path.join(TRAIN_PDB_BASE_DIR, folder))
        ]
        print(f"ğŸ“ æ‰¾åˆ° {len(pdb_id_folders)} ä¸ªPDBæ–‡ä»¶å¤¹")
    except Exception as e:
        print(f"âŒ é”™è¯¯: è¯»å–PDBç›®å½•å¤±è´¥: {e}")
        return

    # å‡†å¤‡æ‰€æœ‰éœ€è¦å¤„ç†çš„é…ä½“æ•°æ®
    all_ligand_data = []
    
    print("ğŸ” å‡†å¤‡é…ä½“æ•°æ®...")
    for protein_folder_name in tqdm(pdb_id_folders, desc="æ‰«æPDBæ–‡ä»¶å¤¹"):
        pdb_id_for_lookup = protein_folder_name.upper().strip()
        ligand_entries = df_with_pdbid[df_with_pdbid[PDB_ID_COL] == pdb_id_for_lookup]
        
        if ligand_entries.empty:
            continue
        
        for _, ligand_row in ligand_entries.iterrows():
            smiles_str = ligand_row[SMILES_COL]
            seq_id_str = ligand_row[SEQID_COL]
            
            if pd.isna(smiles_str) or smiles_str.strip() == '' or smiles_str == 'nan':
                continue
            
            # æ„å»ºæ–‡ä»¶è·¯å¾„ - ä½¿ç”¨ä¸BindingDBä¸€è‡´çš„å‘½åæ ¼å¼ PDBID_SEQID
            sanitized_seqid = sanitize_name_component(seq_id_str)
            ligand_folder_name = f"{protein_folder_name}_{sanitized_seqid}"  # PDBID_SEQIDæ ¼å¼
            ligand_folder_path = os.path.join(TRAIN_PDB_BASE_DIR, protein_folder_name, ligand_folder_name)
            sdf_file_path = os.path.join(ligand_folder_path, f"{ligand_folder_name}.sdf")
            
            all_ligand_data.append((pdb_id_for_lookup, seq_id_str, smiles_str, sdf_file_path))
    
    print(f"ğŸ“Š æ€»å…±éœ€è¦å¤„ç† {len(all_ligand_data)} ä¸ªé…ä½“")
    
    if not all_ligand_data:
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„é…ä½“æ•°æ®")
        return
    
    # åˆ†æ‰¹å¤„ç†
    batches = [all_ligand_data[i:i + BATCH_SIZE] for i in range(0, len(all_ligand_data), BATCH_SIZE)]
    print(f"ğŸ“¦ åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡å¤„ç†")
    
    # ç»Ÿè®¡ç»“æœ
    total_success = 0
    total_failed = 0
    total_skipped = 0
    
    start_time = time.time()
    
    # ä½¿ç”¨å•è¿›ç¨‹å¤„ç† (é¿å…tmuxä¸­çš„å¤šè¿›ç¨‹é—®é¢˜)
    print("ğŸ”„ ä½¿ç”¨å•è¿›ç¨‹å¤„ç† (è·³è¿‡å‡ ä½•ä¼˜åŒ–ï¼Œä¸ä¹‹å‰æ•°æ®é›†ä¿æŒä¸€è‡´)...")
    results = []

    # ä½¿ç”¨tqdmè¿›åº¦æ¡
    with tqdm(total=len(batches), desc="å¤„ç†æ‰¹æ¬¡", unit="batch") as pbar:
        for i, batch in enumerate(batches):
            # æ›´æ–°è¿›åº¦æ¡æè¿°
            pbar.set_description(f"å¤„ç†æ‰¹æ¬¡ {i+1}/{len(batches)}")

            result = process_ligand_batch(batch, i+1)
            results.append(result)

            # æ›´æ–°è¿›åº¦æ¡åç¼€ä¿¡æ¯
            temp_success = sum(r['success'] for r in results)
            temp_failed = sum(r['failed'] for r in results)
            temp_skipped = sum(r['skipped'] for r in results)
            elapsed = time.time() - start_time
            rate = (temp_success + temp_failed) / elapsed if elapsed > 0 else 0

            pbar.set_postfix({
                'æˆåŠŸ': temp_success,
                'å¤±è´¥': temp_failed,
                'è·³è¿‡': temp_skipped,
                'é€Ÿåº¦': f"{rate:.1f}/ç§’"
            })

            # æ›´æ–°è¿›åº¦æ¡
            pbar.update(1)

            # æ¯å¤„ç†20ä¸ªæ‰¹æ¬¡æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
            if (i + 1) % 20 == 0:
                print(f"\nğŸ“Š é˜¶æ®µç»Ÿè®¡ (æ‰¹æ¬¡{i+1}): æˆåŠŸ={temp_success}, å¤±è´¥={temp_failed}, è·³è¿‡={temp_skipped}, å¹³å‡é€Ÿåº¦={rate:.2f}/ç§’")
    
    # æ±‡æ€»ç»“æœ
    for result in results:
        total_success += result['success']
        total_failed += result['failed']
        total_skipped += result['skipped']
    
    elapsed_time = time.time() - start_time
    
    print(f"\nğŸ‰ å¤„ç†å®Œæˆ!")
    print(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time:.1f} ç§’")
    print(f"âœ… æˆåŠŸç”Ÿæˆ: {total_success} ä¸ªSDFæ–‡ä»¶")
    print(f"â© è·³è¿‡å·²å­˜åœ¨: {total_skipped} ä¸ªæ–‡ä»¶")
    print(f"âŒ ç”Ÿæˆå¤±è´¥: {total_failed} ä¸ªæ–‡ä»¶")
    print(f"ğŸ“ è¯¦ç»†å¤±è´¥ä¿¡æ¯è¯·æŸ¥çœ‹: {FAILURE_LOG_FILE}")
    
    if total_success > 0:
        avg_time = elapsed_time / (total_success + total_failed)
        print(f"ğŸ“Š å¹³å‡å¤„ç†é€Ÿåº¦: {avg_time:.2f} ç§’/ä¸ª")

if __name__ == '__main__':
    main()
