import os
import pandas as pd
import asyncio
import aiohttp
from tqdm import tqdm
import time

# å…¨å±€é…ç½®
MAX_CONCURRENT = 20  # é™ä½å¹¶å‘æ•°
MAX_RETRIES = 3     # æœ€å¤§é‡è¯•æ¬¡æ•°
TIMEOUT = 60        # å¢åŠ è¶…æ—¶æ—¶é—´
RETRY_DELAY = 1     # é‡è¯•é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰

async def download_pdb(session, pdb_id, full_pdb_id, pdb_dir, semaphore):
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    pdb_path = os.path.join(pdb_dir, f"{full_pdb_id}.pdb")
    if os.path.exists(pdb_path):
        return f"â© {full_pdb_id}: æ–‡ä»¶å·²å­˜åœ¨"

    url = f"https://files.rcsb.org/download/{pdb_id}.pdb"
    
    for retry in range(MAX_RETRIES):
        try:
            async with semaphore:
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=TIMEOUT)) as response:
                    if response.status == 200:
                        content = await response.read()
                        os.makedirs(pdb_dir, exist_ok=True)
                        with open(pdb_path, 'wb') as f:
                            f.write(content)
                        return f"âœ… {full_pdb_id}: ä¸‹è½½æˆåŠŸ"
                    elif response.status == 404:
                        return f"âŒ {full_pdb_id}: PDBæ–‡ä»¶ä¸å­˜åœ¨ (404)"
                    else:
                        if retry < MAX_RETRIES - 1:
                            await asyncio.sleep(RETRY_DELAY)
                            continue # è¿›å…¥ä¸‹ä¸€æ¬¡é‡è¯•
                        return f"âŒ {full_pdb_id}: HTTP {response.status} (å·²é‡è¯• {retry + 1} æ¬¡)"
        except asyncio.TimeoutError:
            if retry < MAX_RETRIES - 1:
                await asyncio.sleep(RETRY_DELAY)
                continue # è¿›å…¥ä¸‹ä¸€æ¬¡é‡è¯•
            return f"âŒ {full_pdb_id}: ä¸‹è½½è¶…æ—¶ (å·²é‡è¯• {retry + 1} æ¬¡)"
        except Exception as e:
            if retry < MAX_RETRIES - 1:
                await asyncio.sleep(RETRY_DELAY)
                continue # è¿›å…¥ä¸‹ä¸€æ¬¡é‡è¯•
            return f"âŒ {full_pdb_id}: {str(e)} (å·²é‡è¯• {retry + 1} æ¬¡)"
    
    return f"âŒ {full_pdb_id}: æ‰€æœ‰ {MAX_RETRIES} æ¬¡é‡è¯•å‡å¤±è´¥"

async def main():
    start_time = time.time()
    
    # è®¾ç½®ä¸»ç›®å½•
    main_dir = 'biosnap/train_pdb'
    os.makedirs(main_dir, exist_ok=True)

    # è¯»å–CSVæ–‡ä»¶
    try:
        df = pd.read_csv('biosnap/train_csv/with_pdbid.csv')
        if 'PDB_ID' not in df.columns:
            print("âŒ é”™è¯¯ï¼šCSVæ–‡ä»¶ä¸­æ‰¾ä¸åˆ° 'PDB_ID' åˆ—")
            return
    except FileNotFoundError:
        print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° biosnap/train_csv/with_pdbid.csv æ–‡ä»¶")
        return
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼šè¯»å–CSVæ–‡ä»¶æ—¶å‡ºé”™ - {str(e)}")
        return

    # æ£€æŸ¥CSVä¸­çš„å”¯ä¸€PDB IDæ•°é‡
    unique_pdb_ids = df['PDB_ID'].nunique()
    total_rows = len(df)
    print(f"â„¹ï¸ CSVæ–‡ä»¶æ€»è¡Œæ•°: {total_rows}")
    print(f"â„¹ï¸ CSVæ–‡ä»¶ä¸­å”¯ä¸€çš„PDB IDæ•°é‡: {unique_pdb_ids}")
    if total_rows > unique_pdb_ids:
        print(f"âš ï¸ è­¦å‘Š: CSVæ–‡ä»¶ä¸­åŒ…å« {total_rows - unique_pdb_ids} ä¸ªé‡å¤çš„PDB IDæ¡ç›®ã€‚")

    print(f"ğŸ“‹ æ€»å…±éœ€è¦å¤„ç† {total_rows} ä¸ªPDBæ¡ç›®")
    
    semaphore = asyncio.Semaphore(MAX_CONCURRENT)
    connector = aiohttp.TCPConnector(limit=MAX_CONCURRENT, force_close=False, enable_cleanup_closed=True)
    client_timeout = aiohttp.ClientTimeout(total=TIMEOUT)
    
    async with aiohttp.ClientSession(connector=connector, timeout=client_timeout) as session:
        tasks = []
        for index, row in df.iterrows():
            # å»é™¤PDB_IDå‰åå¯èƒ½å­˜åœ¨çš„ç©ºæ ¼ï¼Œå¹¶ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹
            full_pdb_id = str(row['PDB_ID']).strip()
            
            if not full_pdb_id: # è·³è¿‡ç©ºçš„PDB_ID
                print(f"âš ï¸ è­¦å‘Š: ç¬¬ {index + 1} è¡Œçš„ PDB_ID ä¸ºç©ºï¼Œè·³è¿‡")
                continue

            if len(full_pdb_id) < 4 or not full_pdb_id[:4].isalnum():
                print(f"âš ï¸ è­¦å‘Š: ç¬¬ {index + 1} è¡Œçš„ PDB_ID '{full_pdb_id}' æ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡")
                continue
            
            pdb_id = full_pdb_id[:4].lower()
            pdb_dir = os.path.join(main_dir, full_pdb_id) 
            
            task = download_pdb(session, pdb_id, full_pdb_id, pdb_dir, semaphore)
            tasks.append(task)
        
        print("ğŸš€ å¼€å§‹ä¸‹è½½...")
        results = []
        failed_downloads = []
        
        with tqdm(total=len(tasks), desc="ğŸ“¥ ä¸‹è½½è¿›åº¦") as pbar:
            for f_completed in asyncio.as_completed(tasks):
                result = await f_completed
                results.append(result)
                if "âŒ" in result:
                    failed_downloads.append(result)
                pbar.update(1)
                if "âŒ" in result:
                     # é¿å…åœ¨tqdmè¿›åº¦æ¡åˆ·æ–°æ—¶ç ´åè¾“å‡ºæ ¼å¼
                    pbar.write(f"{result}") 
    
    success = len([r for r in results if "âœ…" in r])
    skipped = len([r for r in results if "â©" in r])
    failed = len([r for r in results if "âŒ" in r])
    
    end_time = time.time()
    duration = end_time - start_time
    
    print("\nğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
    print(f"âœ… æˆåŠŸ: {success}")
    print(f"â© è·³è¿‡ (æ–‡ä»¶å·²å­˜åœ¨): {skipped}")
    print(f"âŒ å¤±è´¥: {failed}")
    print(f"â±ï¸ æ€»è€—æ—¶: {duration:.2f} ç§’")
    if len(results) > 0 and duration > 0:
        print(f"ğŸš€ å¹³å‡é€Ÿåº¦: {len(results)/duration:.2f} ä¸ªæ¡ç›®/ç§’")
    
    if failed_downloads:
        with open('failed_downloads.txt', 'w') as f_err:
            f_err.write('\n'.join(failed_downloads))
        print(f"\nâ— å¤±è´¥çš„ä¸‹è½½å·²ä¿å­˜åˆ° failed_downloads.txt")

if __name__ == "__main__":
    # ç¡®ä¿åœ¨Windowsä¸Šä¹Ÿèƒ½æ­£å¸¸è¿è¡Œasyncioçš„tqdm
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())